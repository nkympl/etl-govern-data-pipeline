# Arquitetura do Projeto - ETL govern data pipeline

Este documento descreve a arquitetura tÃ©cnica e o fluxo de dados do projeto **ETL govern data pipeline**, uma simulaÃ§Ã£o do projeto de pipeline corporativo do qual fiz parte durante meu estÃ¡gio no MinistÃ©rio da JustiÃ§a e SeguranÃ§a PÃºblica para tratamento e integraÃ§Ã£o de dados de compras pÃºblicas utilizando Node.js, JSON, PostgreSQL e Power BI.

---

## MÃ³dulo 1 - Estrutura Inicial do Projeto

### Objetivo

Definir a **arquitetura de diretÃ³rios e arquivos** que organiza o projeto de forma modular, clara e escalÃ¡vel.

### DescriÃ§Ã£o

O projeto Ã© estruturado para refletir as etapas clÃ¡ssicas de um pipeline ETL (_Extract, Transform, Load_), bem como separar documentaÃ§Ã£o, dados e scripts de forma lÃ³gica.

### Estrutura

```text
ğŸ“¦ etl-govern-data-pipeline
â”œâ”€â”€ ğŸ“ data
â”‚ â”œâ”€â”€ ğŸ“ raw â†’ planilhas originais (entrada)
â”‚ â””â”€â”€ ğŸ“ processed â†’ dados limpos e padronizados
â”‚
â”œâ”€â”€ ğŸ“ etl
â”‚ â”œâ”€â”€ extract.js â†’ cÃ³digo de extraÃ§Ã£o dos arquivos Excel
â”‚ â”œâ”€â”€ transform.js â†’ cÃ³digo de limpeza e padronizaÃ§Ã£o
â”‚ â””â”€â”€ load.js â†’ cÃ³digo de inserÃ§Ã£o no banco
â”‚
â”œâ”€â”€ ğŸ“ db
â”‚ â””â”€â”€ schema.sql â†’ script SQL de criaÃ§Ã£o das tabelas
â”‚
â”œâ”€â”€ ğŸ“ docs
â”‚ â””â”€â”€ arquitetura.md â†’ documentaÃ§Ã£o tÃ©cnica
â”‚
â”œâ”€â”€ .env.example â†’ variÃ¡veis de ambiente
â”œâ”€â”€ package.json â†’ dependÃªncias e scripts
â””â”€â”€ README.md â†’ guia geral do projeto
```

### Tecnologias instaladas

- `xlsx` â†’ leitura e escrita de planilhas Excel
- `pg` â†’ integraÃ§Ã£o com PostgreSQL
- `dotenv` â†’ gerenciamento de variÃ¡veis de ambiente

### Resultados

Ambiente inicial funcional com estrutura modular e dependÃªncias configuradas.

## MÃ³dulo 1.5 - ConfiguraÃ§Ã£o de Ambiente

### Objetivo

Definir variÃ¡veis de ambiente que parametrizam a execuÃ§Ã£o do pipeline sem expor dados sensÃ­veis.

### Estrutura do `.env.example`

```bash
# ConfiguraÃ§Ãµes do Banco de Dados PostgreSQL
DB_HOST=localhost
DB_PORT=5432
DB_NAME=etl_govern_data_pipeline
DB_USER=postgres
DB_PASSWORD=senha_segura

# Caminhos internos
RAW_DATA_PATH=./data/raw
PROCESSED_DATA_PATH=./data/processed

# Ambiente de execuÃ§Ã£o
NODE_ENV=development
```

### ObservaÃ§Ã£o sobre o ambiente de desenvolvimento

Este projeto foi criado e executado em ambiente **GitHub Codespaces**, e o banco de dados PostgreSQL roda em um contÃªiner Docker separado.  
Essa abordagem simula a estrutura de deploy em microserviÃ§os, onde o backend e o banco residem em contÃªineres distintos, comunicando-se via rede interna.

**ConfiguraÃ§Ã£o Docker usada:**

```bash
docker run --name postgres -e POSTGRES_PASSWORD=123456 -p 5432:5432 -d postgres
```

### Nota sobre comunicaÃ§Ã£o Node â†” PostgreSQL no Codespaces

Durante os testes de conexÃ£o, verifiquei que o Codespaces executa o Node.js diretamente no ambiente base,
enquanto o PostgreSQL roda em um container Docker separado.

Isso significa que a comunicaÃ§Ã£o entre o Node e o banco **ocorre via porta publicada no host (localhost)**, e nÃ£o pela rede Docker interna.

**ConfiguraÃ§Ã£o final adotada no .env:**

```bash
DB_HOST=localhost
DB_PORT=5432
```

## MÃ³dulo 2 - ExtraÃ§Ã£o dos Dados (E)

### Objetivo

Ler planilhas Excel contendo dados de compras pÃºblicas e converter seu conteÃºdo em objetos manipulÃ¡veis (JSON) dentro do Node.js.

### DescriÃ§Ã£o

A etapa de ExtraÃ§Ã£o (Extract) coleta dados brutos das fontes originais, neste caso, arquivos Excel localizados em `data/raw/`.
O script `etl/extract.js` lÃª cada planilha, seleciona a primeira aba e transforma as linhas em um array de objetos JavaScript.

### Ferramentas e mÃ©todos

- Biblioteca `xlsx`
- FunÃ§Ãµes principais:
  - `readFile()` - carrega a planilha;
  - `sheet_to_json()` - converte a aba em JSON.

### Fluxo lÃ³gico

```text
ğŸ“ data/raw/compras_sp.xlsx
   â†“
Node.js + XLSX â†’ Leitura e ConversÃ£o
   â†“
Array de Objetos JSON â†’ SaÃ­da no Console
```

### MÃ³dulo 3 - Etapa de TransformaÃ§Ã£o (T)

ResponsÃ¡vel por:

- Padronizar nomes de colunas (remoÃ§Ã£o de acentos e uso de snake_case);
- Calcular o campo `valor_total`;
- Gerar arquivos JSON limpos prontos para carga no banco de dados.

**Entrada:** planilhas Excel convertidas em JSON.  
**SaÃ­da:** arquivos JSON padronizados em `/data/processed`.

## MÃ³dulo 4 - CriaÃ§Ã£o e ConexÃ£o com o Banco de Dados (L - Parte 1)

### Objetivo

Estabelecer a camada de persistÃªncia do pipeline ETL utilizando PostgreSQL, definindo o schema e testando a conexÃ£o com o Node.js.

### Estrutura criada

Arquivo `db/schema.sql` com definiÃ§Ã£o da tabela `compras_publicas`:

- **id:** chave primÃ¡ria sequencial;
- **uf, orgao, item:** campos de identificaÃ§Ã£o e descriÃ§Ã£o;
- **quantidade, valor_unitario:** campos numÃ©ricos com validaÃ§Ã£o;
- **valor_total:** coluna gerada automaticamente pelo banco;
- **data_insercao:** registro de data e hora de inserÃ§Ã£o.

### ConexÃ£o

Arquivo `db/connection.js` implementa a integraÃ§Ã£o Node â†” PostgreSQL usando o mÃ³dulo `pg` e variÃ¡veis de ambiente definidas em `.env`.

### Resultado esperado

ExecuÃ§Ã£o do script confirma a conexÃ£o com o banco exibindo:

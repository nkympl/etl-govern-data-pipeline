# arquitetura do projeto - etl govern data pipeline

este documento descreve a arquitetura e o funcionamento do projeto etl govern data pipeline, uma simulaÃ§Ã£o prÃ¡tica e em versÃ£o mais bÃ¡sica, inspirada em cenÃ¡rios reais de integraÃ§Ã£o de dados corporativos durante meu estÃ¡gio no ministÃ©rio da justiÃ§a e seguranÃ§a pÃºblica. o objetivo Ã© demonstrar o funcionamento completo de um pipeline etl (extract, transform, load) com node.js, json, postgresql, power bi e, com o meu progresso nos estudos, uma api feita com next.js mais pra frente (se deus quiser haha).

### mÃ³dulo 1 - estrutura inicial do projeto

### objetivo

criar a estrutura de diretÃ³rios e arquivos que organiza o projeto em etapas claras do etl.

```text
etl-govern-data-pipeline
â”œâ”€â”€ ğŸ“ data
â”‚ â”œâ”€â”€ ğŸ“ raw â†’ planilhas originais (entrada)
â”‚ â””â”€â”€ ğŸ“ processed â†’ dados limpos e padronizados
â”‚
â”œâ”€â”€ ğŸ“ etl
â”‚ â”œâ”€â”€ extract.js â†’ cÃ³digo de extraÃ§Ã£o dos arquivos Excel
â”‚ â”œâ”€â”€ transform.js â†’ cÃ³digo de limpeza e padronizaÃ§Ã£o
â”‚ â””â”€â”€ load.js â†’ cÃ³digo de inserÃ§Ã£o no banco
â”‚
â”œâ”€â”€ ğŸ“ db
â”‚ â””â”€â”€ schema.sql â†’ script SQL de criaÃ§Ã£o das tabelas
â”‚
â”œâ”€â”€ ğŸ“ docs
â”‚ â””â”€â”€ arquitetura.md â†’ documentaÃ§Ã£o tÃ©cnica
â”‚
â”œâ”€â”€ .env.example â†’ variÃ¡veis de ambiente
â”œâ”€â”€ package.json â†’ dependÃªncias e scripts
â””â”€â”€ README.md â†’ guia geral do projeto
```

### dependÃªncias principais

- `xlsx` â†’ leitura e escrita de planilhas Excel
- `pg` â†’ integraÃ§Ã£o com PostgreSQL
- `dotenv` â†’ gerenciamento de variÃ¡veis de ambiente

### mÃ³dulo 1.5 - configuraÃ§Ã£o de ambiente e docker no codespaces

### o que foi feito?

inicialmente o projeto rodava no github codespaces, e o banco postgresql estava em um container docker.

```bash
docker run --name postgres-db -e POSTGRES_PASSWORD=123456 -p 5432:5432 -d postgres
```

o node.js rodava direto no codespaces, entÃ£o a conexÃ£o com o banco era feita pelo host `localhost` e porta `5432`.

```bash
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=123456

```

### dificuldade encontrada

durante os testes, o container do postgresql passou a apresentar erros de inicializaÃ§Ã£o e travamentos no codespaces. o banco parava de responder e nÃ£o mantinha os dados entre execuÃ§Ãµes.

isso mostrou a limitaÃ§Ã£o do ambiente remoto para esse tipo de uso e levou Ã  decisÃ£o de migrar o projeto para a mÃ¡quina local, usando o postgresql instalado diretamente no sistema.

depois dessa migraÃ§Ã£o, a conexÃ£o passou a funcionar corretamente e a pipeline ficou estÃ¡vel.

### mÃ³dulo 2 - extraÃ§Ã£o (extract)

### objetivo

ler as planilhas excel de compras pÃºblicas e convertÃª-las em json.

### funcionamento

- o script extract.js lÃª todos os arquivos .xlsx em data/raw/
- converte cada um para .json com o mesmo nome
- salva os arquivos json na mesma pasta

essa etapa foi adaptada para lidar com mÃºltiplas planilhas de diferentes ufs, mantendo a execuÃ§Ã£o simples.

### mÃ³dulo 3 - transformaÃ§Ã£o (transform)

### objetivo

limpar e padronizar os dados, transformando as chaves em snake_case e garantindo que os valores numÃ©ricos sejam vÃ¡lidos.

### funcionamento

- percorre todos os arquivos .json em data/raw
- aplica normalizaÃ§Ã£o nas chaves e converte quantidade e valor_unitario em nÃºmero
- gera arquivos \_normalized.json em data/processed

### mÃ³dulo 4 - criaÃ§Ã£o e conexÃ£o do banco de dados

### objetivo

criar o banco de dados etl_govern_data_pipeline e a tabela compras_publicas no postgresql.

### estrutura da tabela

- id serial primary key
- uf, orgao, item
- quantidade, valor_unitario
- valor_total (gerado automaticamente: GENERATED ALWAYS AS (quantidade \* valor_unitario) STORED)
- data_insercao (timestamp)

### resultado

a conexÃ£o node â†” postgresql foi validada com sucesso apÃ³s a migraÃ§Ã£o para o ambiente local.

### mÃ³dulo 5 - carga (load)

### objetivo

inserir os dados processados no banco de forma segura e controlada.

### principais decisÃµes

1. uso de transaÃ§Ã£o (BEGIN, COMMIT, ROLLBACK)
2. exclusÃ£o do campo valor_total no insert, pois o postgresql calcula automaticamente
3. uso de lotes de inserÃ§Ã£o para performance
4. verificaÃ§Ã£o de campos obrigatÃ³rios antes da carga

### evoluÃ§Ã£o do mÃ³dulo

inicialmente o script gerava erro de permissÃ£o e conflito de schema. isso foi resolvido ajustando as permissÃµes no banco.
depois houve um erro com o campo valor_total, resolvido ao remover o cÃ¡lculo do transform.js e deixar o banco gerar automaticamente.

posteriormente, o pipeline foi ajustado para:

- evitar duplicaÃ§Ãµes (ON CONFLICT DO NOTHING)
- e depois, para refletir alteraÃ§Ãµes de planilha (ON CONFLICT DO UPDATE)

### resultado

ao alterar dados em uma planilha e executar novamente o etl, o banco agora atualiza automaticamente quantidade, valor_unitario e data_insercao.

### mÃ³dulo 6 - idempotÃªncia e monitoramento

### objetivo

garantir que o etl possa rodar vÃ¡rias vezes sem causar duplicaÃ§Ãµes e registrar o histÃ³rico de execuÃ§Ã£o.

### implementaÃ§Ãµes

- uso de ON CONFLICT para prevenir duplicatas
- criaÃ§Ã£o de constraint UNIQUE (uf, orgao, item)
- limpeza das duplicatas antigas no banco
- logs simples no console para indicar sucesso e erro

a tabela de controle etl_execucoes foi planejada mas ainda nÃ£o implementada. a idempotÃªncia Ã© garantida pela constraint e pelo tratamento de conflito.

### mÃ³dulo 7 - integraÃ§Ã£o com power bi

### objetivo

conectar o banco postgresql ao power bi e montar um painel simples.

### configuraÃ§Ã£o

- conexÃ£o via modo importar (localhost, banco etl_govern_data_pipeline)
- tabela usada: compras_publicas
- visuais:
  - cartÃ£o com soma de valor_total
  - grÃ¡fico de barras (valor_total por uf)
  - grÃ¡fico de colunas (valor_total por orgao)
  - tabela detalhada
  - filtro por uf

### observaÃ§Ãµes

- o modo importar precisa de atualizaÃ§Ã£o manual (botÃ£o atualizar)
- o modo directquery pode ser usado para dados em tempo real, mas Ã© mais lento
- o painel foi montado de forma simples e funcional, com layout limpo e intuitivo

### mÃ³dulo 8 - api do etl com next.js (a ser estudado)

### objetivo

disponibilizar o pipeline etl por meio de rotas http usando o sistema de api do next.js.

### script auxiliar - run-etl.js

foi criado um script simples para executar todo o pipeline de uma vez, sem precisar chamar os trÃªs scripts separadamente.
